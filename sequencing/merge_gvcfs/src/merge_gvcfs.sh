#!/bin/bash
# call_genotypes 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# Your job's input variables (if any) will be loaded as environment
# variables before this script runs.  Any array inputs will be loaded
# as bash arrays.
#
# Any code outside of main() (or any entry point you may add) is
# ALWAYS executed, followed by running the entry point itself.
#
# See https://wiki.dnanexus.com/Developer-Portal for tutorials on how
# to modify this file.


# install GNU parallel!
sudo sed -i 's/^# *\(deb .*backports.*\)$/\1/' /etc/apt/sources.list
sudo apt-get update
sudo apt-get install --yes parallel

sudo pip install pytabix

set -x

function download_resources() {

	# get the resources we need in /usr/share/GATK
	sudo mkdir -p /usr/share/GATK/resources
	sudo chmod -R a+rwX /usr/share/GATK

	dx download $(dx find data --name "GenomeAnalysisTK-3.3-0.jar" --project $DX_RESOURCES_ID --brief) -o /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar
	dx download $(dx find data --name "dbsnp_137.b37.vcf.gz" --project $DX_RESOURCES_ID --folder /resources --brief) -o /usr/share/GATK/resources/dbsnp_137.b37.vcf.gz
	dx download $(dx find data --name "dbsnp_137.b37.vcf.gz.tbi" --project $DX_RESOURCES_ID --folder /resources --brief) -o /usr/share/GATK/resources/dbsnp_137.b37.vcf.gz.tbi
	dx download $(dx find data --name "human_g1k_v37_decoy.fasta" --project $DX_RESOURCES_ID --folder /resources --brief) -o /usr/share/GATK/resources/human_g1k_v37_decoy.fasta
	dx download $(dx find data --name "human_g1k_v37_decoy.fasta.fai" --project $DX_RESOURCES_ID --folder /resources --brief) -o /usr/share/GATK/resources/human_g1k_v37_decoy.fasta.fai
	dx download $(dx find data --name "human_g1k_v37_decoy.dict" --project $DX_RESOURCES_ID --folder /resources --brief) -o /usr/share/GATK/resources/human_g1k_v37_decoy.dict
	
}

function merge_gvcf() {
	set -x
	f=$1

	WKDIR=$3
	cd $WKDIR

	GVCF_LIST=$(mktemp)
		
	TOT_MEM=$(free -m | grep "Mem" | awk '{print $2}')
	N_PROC=$(nproc --all)
	
	while read dx_gvcf; do
		gvcf_fn=$(dx describe --name "$dx_gvcf")
		dx download "$dx_gvcf"
		
		if test "$(echo $gvcf_fn | grep '\.gz$')" -a -z "$(ls $gvcf_fn.tbi)"; then
			tabix -p vcf $gvcf_fn
		fi
		
		echo $gvcf_fn >> $GVCF_LIST
	done < $f
	
	# ask for 150% of per-core memory
	java -d64 -Xms512m -Xmx$((TOT_MEM * 3 / (N_PROC * 2) ))m -jar /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar \
	-T CombineGVCFs \
	-R /usr/share/GATK/resources/human_g1k_v37_decoy.fasta \
	$(cat $GVCF_LIST | sed "s|^|-V |" | tr '\n' ' ') \
	-o "$f.vcf.gz"
	
	rm $GVCF_LIST
	
	echo "$f.vcf.gz" >> $2
	
	# clean up the working directory
	for f in $(cat $GVCF_LIST); do
		rm $f
		rm $f.tbi || true
	done
	
	cd -
	
}
export -f merge_gvcf

# download a GVCf and split it into chromosomal parts
function dl_split() {
	set -x
	dx_f=$1
	WKDIR=$2
	OUTDIR=$3

	fn="$(dx describe --name "$dx_f")"
	fn_base="$(echo $fn | sed 's/^\(.*\)\.vcf\(\.gz\)*$/\1/')"
	cd $WKDIR
	
	dx download "$dx_f" -o "$fn"
	
	if test "$(echo $fn | grep '\.gz$')" -a -z "$(ls $fn.tbi)"; then
		tabix -p vcf $fn
	fi
	
	TOT_MEM=$(free -m | grep "Mem" | awk '{print $2}')
	N_PROC=$(nproc --all)
	
	for chr in  $(tabix -l $fn); do
		java -d64 -Xms512m -Xmx$((TOT_MEM * 3 / (N_PROC * 2) ))m -jar /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar \
		-T SelectVariants -L $chr \
		-V $fn \
		-R /usr/share/GATK/resources/human_g1k_v37_decoy.fasta \
		-o $OUTDIR/$fn_base.$chr.vcf.gz
	done
	
	# clean up the files we were working with
	rm $WKDIR/$fn
	rm $WKDIR/$fn.tbi

}
export -f dl_split

function dl_index() {
	set -x
	cd "$2"
	fn=$(dx describe --name "$1")
	dx download "$1" -o "$fn"
	if test -z "$(ls $fn.tbi)"; then
		tabix -p vcf $fn
	fi
	echo "$2/$fn" >> $3
}
export -f dl_index

function upload_files() {
	set -x
	fn_list=$1
	
	VCF_TMPF=$(mktemp)
	VCFIDX_TMPF=$(mktemp)
	for f in $(cat $fn_list); do
		vcf_fn=$(dx upload --brief $f)
		echo $vcf_fn >> $VCF_TMPF
		vcfidx_fn=$(dx upload --brief $f.tbi)
		echo $vcfidx_fn >> $VCFIDX_TMPF
	done
	
	echo $VCF_TMPF >> $2
	echo $VCFIDX_TMPF >> $3
}

export -f upload_files

function merge_parts() {
	GVCF_LIST=$1
	OUTDIR=$2
	PREFIX=$3
	
	TOT_MEM=$(free -m | grep "Mem" | awk '{print $2}')
	N_PROC=$(nproc --all)

	# Let's oversubscribe the memory by 50% and hope it goes OK
	java -d64 -Xms512m -Xmx$((TOT_MEM * 3 / (N_PROC * 2) ))m -jar /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar \
	-T CombineGVCFs \
	-R /usr/share/GATK/resources/human_g1k_v37_decoy.fasta \
	$(cat $GVCF_LIST | sed "s|^|-V |" | tr '\n' ' ') \
	-o "$OUTDIR/$PREFIX.$CHROM.vcf.gz"
	
}
export -f merge_parts

function dl_merge() {
	DX_GVCF_LIST=$1
	WKDIR=$2
	OUTDIR=$3
	PREFIX=$4
	
	cd $WKDIR
	
	GVCF_LIST=$(mktemp)
	# Download everything in GVCF_LIST to $WKDIR, indexing if necessary
	while read dx_fn; do
		dx download "$dx_fn"
		fn=$(dx describe "$dx_fn" --name)
		echo $fn >> $GVCF_LIST
		if test -z "$(ls $fn.*)"; then
			tabix -p vcf "$fn"
		fi
	done < $DX_GVCF_LIST
		
	CHROM=$(head -1 $GVCF_LIST | sed -e 's/\.vcf\.gz$//' -e 's/^.*\.//')
	
	TOT_MEM=$(free -m | grep "Mem" | awk '{print $2}')
	N_PROC=$(nproc --all)

	java -d64 -Xms512m -Xmx$((TOT_MEM * 3 / (N_PROC * 2) ))m -jar /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar \
	-T CombineGVCFs \
	-R /usr/share/GATK/resources/human_g1k_v37_decoy.fasta -L $CHROM \
	$(cat $GVCF_LIST | sed "s|^|-V |" | tr '\n' ' ') \
	-o "$OUTDIR/$PREFIX.$CHROM.vcf.gz"
	
}
export -f dl_merge

function split_gvcfs() {
	# set the shell to work w/ GNU parallel
	export SHELL="/bin/bash"

	# Arguments:
	# gvcfidxs (optional)
	# array of files, each containing a "dx download"-able file, one per line
	# and the files are tbi indexes of the gvcf.gz files
	# gvcfs (mandatory)
	# array of files, as above, where each line is a single gvcf file
	# PREFIX (mandatory)
	# the prefix to use for the single resultant gvcf
	
	download_resources
	
	# download my gvcfidx_list
	WKDIR=$(mktemp -d)
	cd $WKDIR
	
	if test "$gvcfidx_list"; then
		DX_GVCFIDX_LIST=$(mktemp)
		dx download "$gvcfidx_list" -f -o $DX_GVCFIDX_LIST
	
		while read dxfn; do
			dx download "$dxfn"
		done < $DX_GVCFIDX_LIST
	fi
	
	# OK, now all of the gvcf indexes are in $WKDIR, time to download
	# all of the GVCFs in parallel
	DX_GVCF_LIST=$(mktemp)
	dx download "$gvcfs" -f -o $DX_GVCF_LIST

	OUTDIR=$(mktemp -d)
	# download and split the components into their chromosomal parts
	parallel -u --gnu -j $(nproc --all) dl_split :::: $DX_GVCF_LIST ::: $WKDIR ::: $OUTDIR
	
	# OK, now get a list of the individual files for each chromosome and output that to a file
	GVCF_MASTER_LIST=$(mktemp)
	for chr in $(ls $OUTDIR | grep '\.vcf\.gz$' | sed 's/.*\.\([^.]*\)\.vcf.gz/\1/' | sort | uniq); do
		LIST_TMPF=$(mktemp)
		ls -1 $OUTDIR/*.$chr.vcf.gz > $LIST_TMPF || true
		echo $LIST_TMPF >> $GVCF_MASTER_LIST
	done
	
	cat $GVCF_MASTER_LIST
	
	# upload the files by chromosome (in parallel, please!)
	PROCESS_ARG_FN=$(mktemp)
	DX_GVCF_MASTER_LIST=$(mktemp)
	DX_GVCFIDX_MASTER_LIST=$(mktemp)
	parallel -u -j $(nproc --all) --gnu upload_files :::: $GVCF_MASTER_LIST ::: $DX_GVCF_MASTER_LIST ::: $DX_GVCFIDX_MASTER_LIST
	
	# clean up some temporary files no longer used
	while read vcf_fn; do
		rm $vcf_fn
	done < $GVCF_MASTER_LIST

	MASTER_GVCF_OUTPUT=$(mktemp)
	while read dx_gvcf; do
		cat $dx_gvcf >> $MASTER_GVCF_OUTPUT
		rm $dx_gvcf
	done < $DX_GVCF_MASTER_LIST
	
	MASTER_GVCFIDX_OUTPUT=$(mktemp)
	while read dx_gvcfidx; do
		cat $dx_gvcfidx >> $MASTER_GVCFIDX_OUTPUT
		rm $dx_gvcfidx
	done < $DX_GVCFIDX_MASTER_LIST
	
	DX_MASTER_GVCF=$(dx upload $MASTER_GVCF_OUTPUT --brief)
	DX_MASTER_GVCFIDX=$(dx upload $MASTER_GVCFIDX_OUTPUT --brief)
	
	dx-jobutil-add-output gvcfs $DX_MASTER_GVCF --class=file
	dx-jobutil-add-output gvcfidxs $DX_MASTER_GVCFIDX --class=file	
}

function gather_rescatter(){
	# Arguments:
	# gvcfs (mandatory) a list of GVCF files uploaded to DNANexus
	
	IDXFN_FILE=$(mktemp)
	for i in "${!gvcfidxs[@]}"; do
		dx cat "${gvcfidxs[$i]}" >> $IDXFN_FILE
	done
	DX_IDXFN=$(dx upload $IDXFN_FILE --brief)
	
	
	# FN_FILE will have a list of filenames, one per line and the associated dx file identifier
	FN_FILE=$(mktemp)
	
	# describe the gvcfs files
	for i in "${!gvcfs[@]}"; do	
		FN_TMP=$(mktemp)
		dx download "${gvcfs[$i]}" -f -o $FN_TMP
		while read dx_fn; do
			echo "$(dx describe "$dx_fn" --name)$(echo -e "\t")$dx_fn" >> $FN_FILE
		done < $FN_TMP
		
		rm $FN_TMP
	done
	
	# OK, now I want to re-scatter, so get the list of chromosomes
	sed -i 's/^[^\t]*\.\([^.]*\)\.vcf\.gz\t.*$/\1\t&/' $FN_FILE
	cat $FN_FILE
	
	MASTER_CHROM_LIST=$(mktemp)
	for chr in $(cut -f1 $FN_FILE | sort | uniq); do
		echo "Working with Chromosome: $chr"
		FN_CHR=$(mktemp)
		grep "^$chr\W" $FN_FILE | cut -f3 > $FN_CHR
		cat $FN_CHR
		echo $(dx upload $FN_CHR --brief) >> $MASTER_CHROM_LIST
	done
	
	echo "MASTER CHROM LIST:" 
	cat $MASTER_CHROM_LIST
	
	# Now, split the master_chrom_list into pieces with no more than $(nproc) lines
	WKDIR=$(mktemp -d)
	cd $WKDIR
	# also, shuffle the chromosomes so we don't have issues with oversubscribing a node
	cat $MASTER_CHROM_LIST | shuf | split -l $(nproc --all) - "dxfn_list."
	
	# For each dxfn_list.*, start a new merge_chrom job
	CONCAT_ARGS=""
	CIDX=0
	for f in $(ls $WKDIR/dxfn_list.*); do
		subprocess_args=""
		for tmpf in $(cat $f); do
			subprocess_args="$subprocess_args -igvcfs:array:file=$tmpf"
		done
		merge_chrom_job[$CIDX]=$(dx-jobutil-new-job merge_chrom $subprocess_args -igvcfidxs:file=$DX_IDXFN -iPREFIX="$PREFIX")
		CONCAT_ARGS="$CONCAT_ARGS -igvcfidxs:array:file=${merge_chrom_job[$CIDX]}:vcfidx_list -igvcfs:array:file=${merge_chrom_job[$CIDX]}:vcf_list"
		CIDX=$((CIDX + 1))
	done	
	
	# output of gather_rescatter will be an array of chromosomal gVCFs
	concat_job=$(dx-jobutil-new-job concatenate_gvcfs --depends-on ${merge_chrom_job[@]} -iPREFIX="$PREFIX" $CONCAT_ARGS)
	
	dx-jobutil-add-output gvcf "$concat_job:gvcf" --class=jobref
	dx-jobutil-add-output gvcfidx "$concat_job:gvcfidx" --class=jobref


}

function merge_chrom(){

	# set the shell to work w/ GNU parallel
	export SHELL="/bin/bash"

	# arguments:
	# gvcfidxs - single file containing all gvcf indexes we MIGHT need, one per line
	# gvcfs - array of files, each containing all gvcfs for a single chromosome
	# PREFIX - the prefix of the gvcf to use (final name will be $PREFIX.$CHR.vcf.gz)

	# I will have an array of files, each containing all of the gvcfs to merge
	# for a single chromosome
	
	# Also, I'll have a single file with all of the gvcfidxs created - just 
	# download ALL of them, even if they don't apply!
	
	download_resources
	
	WKDIR=$(mktemp -d)
	
	# download ALL of the indexes!
	GVCFIDX_FN=$(mktemp)
	dx download "$gvcfidxs" -f -o $GVCFIDX_FN
	
	cd $WKDIR
	while read dx_fn; do
		dx download "$dx_fn"
	done < $GVCFIDX_FN
	
	# OK, now all of the gvcf indexes are in $WKDIR, time to download
	# all of the GVCFs in parallel
	DX_GVCF_LIST=$(mktemp)
	for i in "${!gvcfs[@]}"; do	
		GVCF_CHR=$(mktemp)
		dx download "${gvcfs[$i]}" -f -o $GVCF_CHR
		echo $GVCF_CHR >> $DX_GVCF_LIST
	done
		

	OUTDIR=$(mktemp -d)
	# download the files, one per chromosome and merge them!
	parallel -u --gnu -j $(nproc --all) dl_merge :::: $DX_GVCF_LIST ::: $WKDIR ::: $OUTDIR ::: $PREFIX
	
	DX_GVCF_LIST=$(mktemp)
	DX_GVCFIDX_LIST=$(mktemp)
	# Now, upload each file and its associated index, adding that file ID to the correct file
	for f in $(ls $OUTDIR/*.vcf.gz); do
		echo $(dx upload $f --brief) >> $DX_GVCF_LIST
	done
	
	for f in $(ls $OUTDIR/*.vcf.gz.tbi); do
		echo $(dx upload $f --brief) >> $DX_GVCFIDX_LIST
	done
	
	# now, upload those lists, and add them to the output here
	VCF_OUT=$(dx upload $DX_GVCF_LIST --brief)
	VCFIDX_OUT=$(dx upload $DX_GVCFIDX_LIST --brief)
	
	dx-jobutil-add-output vcf_list "$VCF_OUT"
	dx-jobutil-add-output vcfidx_list "$VCFIDX_OUT"

}	

function concatenate_gvcfs(){
	# set the shell to work w/ GNU parallel
	export SHELL="/bin/bash"

	# Arguments:
	# gvcfidxs (optional)
	# array of files, each containing a "dx download"-able file, one per line
	# and the files are tbi indexes of the gvcf.gz files
	# gvcfs (mandatory)
	# array of files, as above, where each line is a single gvcf file
	# PREFIX (mandatory)
	# the prefix to use for the single resultant gvcf

	download_resources
	
	# download my gvcfidx_list
	DX_GVCFIDX_LIST=$(mktemp)
	WKDIR=$(mktemp -d)

	for i in "${!gvcfidxs[@]}"; do	
		dx cat "${gvcfidxs[$i]}" >> $DX_GVCFIDX_LIST
	done
	
	cd $WKDIR
	while read dxfn; do
		dx download "$dxfn"
	done < $DX_GVCFIDX_LIST
	
	# OK, now all of the gvcf indexes are in $WKDIR, time to download
	# all of the GVCFs in parallel
	DX_GVCF_LIST=$(mktemp)
	for i in "${!gvcfs[@]}"; do	
		dx cat "${gvcfs[$i]}" >> $DX_GVCF_LIST
	done
	
	# download (and index if necessary) all of the gVCFs
	GVCF_LIST=$(mktemp)	
	parallel -u --gnu -j $(nproc --all) dl_index :::: $DX_GVCF_LIST ::: $WKDIR ::: $GVCF_LIST
	
	# Now, merge the gVCFs into a single gVCF
	FINAL_DIR=$(mktemp -d)
	TOT_MEM=$(free -m | grep "Mem" | awk '{print $2}')
	java -d64 -Xms512m -Xmx$((TOT_MEM * 9 / 10))m -jar /usr/share/GATK/GenomeAnalysisTK-3.3-0.jar \
	-T CombineVariants -nt $(nproc --all) --assumeIdenticalSamples \
	$(cat $GVCF_LIST | sed 's/^/-V /' | tr '\n' ' ') \
	-R /usr/share/GATK/resources/human_g1k_v37_decoy.fasta \
	-genotypeMergeOptions UNSORTED \
	-o $FINAL_DIR/$PREFIX.vcf.gz
	
	# and upload it and we're done!
	DX_GVCF_UPLOAD=$(dx upload "$FINAL_DIR/$PREFIX.vcf.gz" --brief)
	DX_GVCFIDX_UPLOAD=$(dx upload "$FINAL_DIR/$PREFIX.vcf.gz.tbi" --brief)
	
	dx-jobutil-add-output gvcf $DX_GVCF_UPLOAD --class=file
	dx-jobutil-add-output gvcfidx $DX_GVCFIDX_UPLOAD --class=file
	
}


# entry point for merging into a single gVCF
function single_merge_subjob() {

	# set the shell to work w/ GNU parallel
	export SHELL="/bin/bash"
	
	# If we are working with both GVCFs and their index files, let's break it up by interval
	# If no interval given, just break up by chromosome
	if test "$gvcfs" -a "$gvcfidxs"; then
	
		OVER_SUB=1
		if test "$target"; then
			OVER_SUB=2
			dx cat $target | interval_pad $padding
		fi
	
	else
	
	SPLIT_ARGS=""
	if test "$gvcfidxs"; then
		gvcfidxfn=$(dx describe "$gvcfidxs" --json | jq .id | sed 's/"//g')
		SPLIT_ARGS="$SPLIT_ARGS -igvcfidx_list:file=$gvcfidxfn"
	fi

	LIST_DIR=$(mktemp -d)
	dx download "$gvcflist" -o $LIST_DIR/GVCF_LIST
	
	GVCF_WKDIR=$(mktemp -d)	
	
	# instead of subbing by core, split the input into files of N lines each!
	SPLIT_WKDIR=$(mktemp -d)
	cd $SPLIT_WKDIR
	cat $LIST_DIR/GVCF_LIST | split -l $(nproc --all) - "gvcf_split."
	cd -
	
	# start subjobs
	CIDX=0
	GATHER_ARGS=""
	for f in $(ls $SPLIT_WKDIR/gvcf_split.*); do
		DX_GVCFLIST=$(dx upload $f --brief)
		split_job[$CIDX]=$(dx-jobutil-new-job split_gvcfs $SPLIT_ARGS -igvcfs:file=$DX_GVCFLIST)
		GATHER_ARGS="$GATHER_ARGS -igvcfs=${split_job[$CIDX]}:gvcfs -igvcfidxs=${split_job[$CIDX]}:gvcfidxs"
		CIDX=$((CIDX + 1))
	done
		
	# gather + rescatter by chromosome
	gather_job=$(dx-jobutil-new-job gather_rescatter --depends-on ${split_job[@]} -iPREFIX="$PREFIX" $GATHER_ARGS)
	
	fi
	
	# output of concatenate_gvcfs will be a single gVCF and accompanying index
	dx-jobutil-add-output gvcf "$gather_job:gvcf" --class=jobref
	dx-jobutil-add-output gvcfidx "$gather_job:gvcfidx" --class=jobref	

}

# entry point for merging VCFs
function merge_subjob() {
	
	# set the shell to work w/ GNU parallel
	export SHELL="/bin/bash"
	
	# Get the prefix from the project, subbing _ for spaces
	if test -z "$PREFIX"; then
		PREFIX="$(dx describe --name $project | sed 's/  */_/g')"
	fi

	LIST_DIR=$(mktemp -d)
	dx download "$gvcflist" -o $LIST_DIR/GVCF_LIST
		
	N_BATCHES=$nbatch
	N_CORES=$(nproc --all)
	
	if test $N_CORES -gt 1 -a $N_BATCHES -eq 1; then
	
		single_args=""
		if test "$gvcfidxs"; then
			echo "Value of Index file: +$gvcfidxs+"
			gvcfidxfn=$(echo "$gvcfidxs" | sed 's/.*\(file-[^"]*\)".*/\1/')
			single_args="$single_args -igvcfidxs:file=$gvcfidxfn"
		fi
		
		if test "$target"; then
			single_args="$single_args -itarget:file=$target"
		fi
	
		single_job=$(dx-jobutil-new-job single_merge_subjob -iPREFIX="$PREFIX" -igvcflist="$gvcflist" $single_args)
		
		# and upload it and we're done!
		dx-jobutil-add-output gvcf1 "$single_job:gvcf" --class=jobref
		dx-jobutil-add-output gvcfidx1 "$single_job:gvcfidx" --class=jobref
			
	else
	
		PREFIX="$PREFIX.$jobidx"
	
		download_resources

		GVCF_TMP=$(mktemp)
		GVCF_TMPDIR=$(mktemp -d)
		
		sudo chmod a+rw $GVCF_TMP
	
		cd  $GVCF_TMPDIR
		
		if test "$gvcfidxs"; then
			DX_GVCFIDX_LIST=$(mktemp)
			dx download "$gvcfidxs" -f -o $DX_GVCFIDX_LIST
		
			while read dxfn; do
				dx download "$dxfn"
			done < $DX_GVCFIDX_LIST
		fi
		
		GVCF_LIST_SHUF=$(mktemp)
		cat $LIST_DIR/GVCF_LIST | shuf  > $GVCF_LIST_SHUF
		split -a $(echo "scale=0; 1+l($N_BATCHES)/l(10)" | bc -l) -n l/$N_BATCHES -d $GVCF_LIST_SHUF "gvcflist."
		cd -
		sudo chmod -R a+rwX $GVCF_TMPDIR

		TMP_GVCF_LIST=$(mktemp)
		ls -1 ${GVCF_TMPDIR}/gvcflist.* > $TMP_GVCF_LIST

		parallel -u -j $N_BATCHES --gnu merge_gvcf :::: $TMP_GVCF_LIST ::: $(echo $GVCF_TMP) ::: $(echo "$project:$folder")
	
		CIDX=1
	
		FINAL_DIR=$(mktemp -d)
	
		for l in $(cat $GVCF_TMP); do
			mv $l ${FINAL_DIR}/$PREFIX.$CIDX.vcf.gz
			mv $l.tbi ${FINAL_DIR}/$PREFIX.$CIDX.vcf.gz.tbi
			
			VCF_DXFN=$(dx upload ${FINAL_DIR}/$PREFIX.$CIDX.vcf.gz --brief)
			VCFIDX_DXFN=$(dx upload ${FINAL_DIR}/$PREFIX.$CIDX.vcf.gz.tbi --brief)
		
			dx-jobutil-add-output gvcf$CIDX "$VCF_DXFN" --class=file
			dx-jobutil-add-output gvcfidx$CIDX "$VCFIDX_DXFN" --class=file
			
			CIDX=$((CIDX + 1))
		done
	fi
}


main() {


	if test -z "$project"; then
		project=$DX_PROJECT_CONTEXT_ID
	fi	

    echo "Value of project: '$project'"  
    echo "Value of folder: '$folder'"
	echo "Value of N_BATCHES: '$N_BATCHES'"
	
	N_GVCF="${#gvcfs[@]}"
    GVCF_LIST=$(mktemp)
    SUBJOB_ARGS=""
	if test "$N_GVCF" -gt 0 ; then
	
		# use the gvcf list provided
		for i in "${!gvcfs[@]}"; do
			echo "${gvcfs[$i]}" >> $GVCF_LIST
		done

		# also, pass the gvcf index list as well!
		if test "${#gvcfidxs[@]}" -gt 0; then
		    GVCFIDX_LIST=$(mktemp)

			for i in "${!gvcfidxs[@]}"; do
				echo "${gvcfidxs[$i]}" >> $GVCFIDX_LIST
			done
			
			dx_gidxlist=$(dx upload $GVCFIDX_LIST --brief)
			SUBJOB_ARGS="$SUBJOB_ARGS -igvcfidxs:file=$dx_gidxlist"
			
			rm $GVCFIDX_LIST
		fi
		
	elif test "$folder"; then
	    for f in $(dx ls $project:$folder | grep '\.gz$'); do
	    	dx describe $project:$folder/$f --json | jq .id
	    done > $GVCF_LIST
	    
	else
		dx-jobutil-report-error "ERROR: you must provide either a list of gvcfs OR a directory containing gvcfs"
	fi
    
    if test "$target"; then
    	SUBJOB_ARGS="$SUBJOB_ARGS -itarget:file=$(echo $target | sed 's/.*\(file-[^"]*\)".*/\1/'))"
    fi
    
    N_GVCF=$(cat $GVCF_LIST | wc -l)
    echo "# GVCF: $N_GVCF"
    
    if test $N_GVCF -le $N_BATCHES; then
    	dx-jobutil-report-error "ERROR: The number of input gVCFs is <= the number of requested output gVCFs, nothing to do!"
    fi	

	# assume that the master instance will filter down to the children    
   	N_CORE_SUBJOB=$(nproc --all)
    N_TEST_BATCHES=$((N_BATCHES / N_CORE_SUBJOB))
    if test $N_TEST_BATCHES -ne 0; then
        N_BATCHES=$N_TEST_BATCHES
    else
    	# o/w, we should use the # of cores to match the # of batches
    	# Note, if N_BATCHES is 1, this will trigger a special logic in the subjob
    	N_CORE_SUBJOB=$N_BATCHES
    fi	
   
    GVCF_TMPDIR=$(mktemp -d)
	cd  $GVCF_TMPDIR
	GVCF_LIST_SHUF=$(mktemp)
	cat $GVCF_LIST | shuf > $GVCF_LIST_SHUF
	split -a $(echo "scale=0; 1+l($N_BATCHES)/l(10)" | bc -l) -n l/$N_BATCHES -d $GVCF_LIST_SHUF "gvcflist."
	cd -
	
	
	
	CIDX=1
	# Now, kick off the subjobs for every file!
	for f in $(ls -1 $GVCF_TMPDIR | sed 's|^.*/||'); do
		# upload the gvcflist
		GVCF_DXFN=$(dx upload $GVCF_TMPDIR/$f --brief)
		# start the sub-job with the project, folder and gvcf list
		subjob=$(dx-jobutil-new-job merge_subjob -iproject="$project" -ifolder="$folder" -igvcflist:file="$GVCF_DXFN" -iPREFIX="$PREFIX" -ijobidx=$CIDX -inbatch=$N_CORE_SUBJOB $SUBJOB_ARGS)
		
		# the output of the subjob will be gvcfN and gvcfidxN, for N=1:#cores
		for c in $(seq 1 $N_CORE_SUBJOB); do
			dx-jobutil-add-output vcf_fn --array "${subjob}:gvcf$c" --class=jobref
			dx-jobutil-add-output vcf_idx_fn --array "${subjob}:gvcfidx$c" --class=jobref
		done		
		
		CIDX=$((CIDX + 1))
		
		# reap the array of gvcf/gvcf_index and add it to the output of this job
	done
}
