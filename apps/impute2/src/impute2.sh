#!/bin/bash
# impute2 0.0.1
# Generated by dx-app-wizard.
#
# Parallelized execution pattern: Your app will generate multiple jobs
# to perform some computation in parallel, followed by a final
# "postprocess" stage that will perform any additional computations as
# necessary.
#
# Your job's input variables (if any) will be loaded as environment
# variables before this script runs.  Any array inputs will be loaded
# as bash arrays.
#
# Any code outside of main() or any other entry point is ALWAYS
# executed, followed by running the entry point itself.
#
# See https://wiki.dnanexus.com/Developer-Portal for tutorials on how
# to modify this file.
set -x
set -o pipefail

sed -i 's/^# *\(deb .*backports.*\)$/\1/' /etc/apt/sources.list
apt-get update
apt-get install --yes parallel

main() {


    # The following line(s) use the dx command-line tool to download your file
    # inputs to the local file system using variable names for the filenames. To
    # recover the original filenames, you can use the output of "dx describe
    # "$variable" --name".

	 echo "Value of input_file: '${haps_input[@]}'"	
 	 echo "Value of chroms: '${chroms[@]}'"	
	#mv ./in/haps_input/* .

    # To report any recognized errors in the correct format in
    # $HOME/job_error.json and exit this script, you can use the
    # dx-jobutil-report-error utility as follows:
    #
    #   dx-jobutil-report-error "My error message"
    #
    # Note however that this entire bash script is executed with -e
    # when running in the cloud, so any line which returns a nonzero
    # exit code will prematurely exit the script; if no error was
    # reported in the job_error.json file, then the failure reason
    # will be AppInternalError with a generic error message.
    #
    # Split your work into parallel tasks.  As an example, the
    # following generates 10 subjobs running with the same dummy
    # input.  The utility dx-jobutil-new-job uses the same syntax as
    # dx run for specifying input, and you can explicitly specify the
    # class to enable proper parsing.

	# Check to make sure that the size of the chromosome and haplotype array are the same
	N_HAPS="${#haps_input[@]}"
	N_CHROM="${#chroms[@]}"
	if test $N_HAPS -ne $N_CHROM; then
		dx-jobutil-report-error "ERROR: The size of the haplotype array and the size of the chromosome array must be identical"
	fi
	
	
	for i in "${!haps_input[@]}"
	do
   		process_jobs[$i]=$(dx-jobutil-new-job imputation -ihaps="${haps_input[$i]}" -ichr="${chroms[$i]}" -iref_folder="$ref" -ispc:int="$snps_per_chunk")
   		
   		# add the imputation job's output to the array of output files printed here
  		dx-jobutil-add-output impute2_out --array "${process_jobs[$i]}:impute2_out" --class=jobref
  		dx-jobutil-add-output info_out --array "${process_jobs[$i]}:info_out" --class=jobref
  		dx-jobutil-add-output summary --array "${process_jobs[$i]}:summary_out" --class=jobref
	done
}

function run_imputation {
	set -x

	CHUNK_START=$(echo $1 | awk '{print $1}'); 
	CHUNK_END=$(echo $1 | awk '{print $2}'); 
	
	echo "Running chunk $1"
	
	prefix="$6"
	OUTPUT_FILE="${prefix}.pos${CHUNK_START}-${CHUNK_END}.best_guess_haps_imputation.impute2"

	# main output file
	#OUTPUT_FILE=${prefix}
    	
	NE=20000

	# try to avoid a race condition by waiting a random amount of time (NTE 30 seconds)
	sleep $((RANDOM % 30))
	
	N_PROC=$8
	# spin-lock while we don't have enough memory, defined as 1/(# processors) fraction free
	# Also, if I'm not running an impute2 job, go ahead and try (and probably fail...)
	echo "Memory Fraction used: $(free | tail -2 | head -1 | sed 's/  */\t/g' | cut -f 3,4 | awk '{print $1 / ($1 + $2)}')"
	while test "$(free | tail -2 | head -1 | sed 's/  */\t/g' | cut -f 3,4 | awk -v ncpu="$N_PROC" '{print (($1 / ($1 + $2)) > (1-1/ncpu))}')" -gt 0 \
	      -a "$(ps -elf | grep impute2 | grep -v grep | wc -l)" -gt 0; do
		# wait at least 10 minutes, and up to an hour
		echo "Sleeping due to low memory:"
		sleep $((600 + RANDOM % 3000))
	done
		
	# Run impute2, and if it failed, write the chunk to a file to re-run
	TOT_MEM=$(free -k | grep "Mem" | awk '{print $2}')
	ulimit -v $((TOT_MEM * 19 / (20 * N_PROC) ))
	
	impute2 -m "$2" -use_prephased_g -known_haps_g "$5" -h "$3" -l "$4" -Ne $NE -int ${CHUNK_START} ${CHUNK_END} \
    	-buffer 250kb -call_thresh 0.9 -allow_large_regions -o $OUTPUT_FILE || echo "$1" >> $7
    
}

export -f run_imputation

imputation() {

	# reference data files GENMAP_FILE
	dx download $(dx find data --name "chr${chr}_map.txt" --project $DX_RESOURCES_ID --folder /$ref_folder --brief) -o genetic_map
	
	#OMNIPS_FILE
	dx download $(dx find data --name "chr${chr}.haplotypes" --project $DX_RESOURCES_ID --folder /$ref_folder --brief) -o ref_haplotypes
	
	#LEGEND_FILE
	dx download $(dx find data --name "chr${chr}.legend" --project $DX_RESOURCES_ID --folder /$ref_folder --brief) -o ref_legend

	# best-guess haplotypes from phasing run
	dx download "$haps" -o sample_haps
	
	#CHUNK file
	# TODO: dynamically generate the chunk file!
	#dx download $(dx find data --name "analysis_chunks_6MB_chr${chr}.txt" --project $DX_PROJECT_CONTEXT_ID --brief) -o chunk_file
	
	# First, how many chunks do I need to get ~56K reference variants per chunk
	# (this is the average # of variants per 5Mb)
	N_VARS=$(cat ref_legend | wc -l)
	N_CHUNKS=$((N_VARS / spc + 1))
	
	# OK, now how many study variants per chunk and how many left over?
	N_SVARS=$(cat sample_haps | wc -l)
	VPC=$((N_SVARS / N_CHUNKS))
	LO=$((N_SVARS % N_CHUNKS))
	
	# Now, get the break points by splitting the study variants into the # per
	# chunk above and get the midpoint bewtween them
	BREAK_FN=$(mktemp)
	cut -d' ' -f3 sample_haps | awk "NR>$VPC-1 && NR < $VPC*$N_CHUNKS && NR % ($VPC + (NR/$VPC < ($LO + 1))) <= 1" | awk "{C=\$1; if(NR % 2 == 1){X=C} else {print int((C+X)/2)} }" > $BREAK_FN
	
	#Now, generate the first chunks by iterating over the break file
	touch chunk_file
	LAST_BREAK=0
	for b in $(cat $BREAK_FN); do
		echo "$((LAST_BREAK + 1)) $b" >> chunk_file
		LAST_BREAK=$b
	done
	
	# get the last position by looking at the tail of the ref_legend file
	LAST_POS=$(tail -n 1 ref_legend | cut -d' ' -f2)
	echo "$((LAST_BREAK + 1)) $((LAST_POS + 1))" >> chunk_file
	
	echo "Contents of chunk file:"
	cat chunk_file	
		
	# Create a temporary directory for everything to live in
	CHUNK_DIR=$(mktemp -d)
	CONCAT_DIR=$(mktemp -d)
	
	# get the prefix of the haplotype file
	prefix="$(dx describe --name "$haps" | sed 's/\..*//').$chr"
	
	export SHELL="/bin/bash"
	
	PREV_CHUNKS=$((N_CHUNKS+1))
	RERUN_FILE=$(mktemp)
	N_RUNS=1
	N_CORES=$(nproc)
	N_JOBS=$(nproc)
	
	# each run, we will decrease the number of cores available until we're at a single core at a time (using ALL the memory)
	while test $N_CHUNKS -gt 0 -a $N_JOBS -gt 0; do	
		N_JOBS=$(echo "$N_CORES/2^($N_RUNS - 1)" | bc)
		# make sure we have a minimum of 1 job, please!
		N_JOBS=$((N_JOBS > 0 ? N_JOBS : 1))

		cat chunk_file | parallel -j $N_JOBS --gnu run_imputation "{}" genetic_map ref_haplotypes ref_legend sample_haps "$CHUNK_DIR/$prefix" $RERUN_FILE $N_JOBS

		PREV_CHUNKS=$N_CHUNKS
		N_CHUNKS=$(cat $RERUN_FILE | wc -l)
		mv $RERUN_FILE chunk_file
		RERUN_FILE=$(mktemp)
		N_RUNS=$((N_RUNS + 1))
		N_JOBS=$((N_JOBS - 1))
	done
	
	if test $N_CHUNKS -gt 0; then
		dx-jobutil-report-error "ERROR: Imputation chunks were no longer making progress.  Error chunks are $(cat $RERUN_FILE | sed 's/.*/[&]/' | tr '\n' ' ')"
	fi
	
	# merge all of the files in the RESULT_DIR, but sort on the 3rd column, please
	# NOTE: we are only sorting files based on the position of the 1st line
	POS_F=$(mktemp)

	for f in ${CHUNK_DIR}/$prefix.*.impute2; do
		# Read the 1st 1,000 bytes and print the 3rd field.  Should eliminate 
		# issues with too many fields for awk to handle
		echo -e "$(head -1 $f | head -c 1000 | awk '{print $3}')\t$f"
	done | tee $POS_F

	POS_SORT_F=$(mktemp)
	sort -t$'\t' -gk1,1 $POS_F > $POS_SORT_F

	while read f; do
		cat "$f"
	done < <(cut -f2- $POS_SORT_F) | pigz > $CONCAT_DIR/$prefix.impute2.gz

	#cat ${CHUNK_DIR}/$prefix.*.impute2 | sort -gk3,3 | pigz > $CONCAT_DIR/$prefix.impute2.gz
	
	#OK, make sure this same logic is implemented for the impute2_info files
	# we MUST match the lines exactly!
	
	NLINE=1
	while read f; do
		# This should print the header line in the first 
		tail -n+${NLINE} "$(echo "$f" | sed 's/$/_info/')"
		NLINE=2
	done < <(cut -f2- $POS_SORT_F) | pigz > $CONCAT_DIR/$prefix.impute2_info.gz

	#cat <(head -1 $(ls ${CHUNK_DIR}/$prefix.*.impute2_info | head -1)) <(for f in ${CHUNK_DIR}/$prefix.*.impute2_info; do tail -n+2 $f; done | sort -gk3,3) | pigz > $CONCAT_DIR/$prefix.impute2_info.gz

	while read f; do
		# This should print the header line in the first 
		cat "$(echo "$f" | sed 's/$/_summary/')"
	done < <(cut -f2- $POS_SORT_F) > $CONCAT_DIR/$prefix.summary

	#cat ${CHUNK_DIR}/$prefix.*.impute2_summary > $CONCAT_DIR/$prefix.summary
	
	impute2_out=$(dx upload $CONCAT_DIR/$prefix.impute2.gz --brief)
	dx-jobutil-add-output impute2_out "$impute2_out" --class=file
	info_out=$(dx upload $CONCAT_DIR/$prefix.impute2_info.gz --brief)
	dx-jobutil-add-output info_out "$info_out" --class=file
	summary_out=$(dx upload $CONCAT_DIR/$prefix.summary --brief)
	dx-jobutil-add-output summary_out "$summary_out" --class=file

	
	echo "Warnings:"
	cat ${CHUNK_DIR}/$prefix.*.impute2_warnings || true
	
}

